
=== LANGUAGE en ===

=== Perplexity result ===
Model: base
Average tokenâ€‘wise loss : 1.9706
Perplexity (exp(loss)) : 7.18
No NaNs
No Infs
ðŸ”Ž Total L2 norm of parameters: 2997.18

=== Perplexity result ===
Model: language_en
Average tokenâ€‘wise loss : 1.8680
Perplexity (exp(loss)) : 6.48
No NaNs
No InfsTotal L2 norm of parameters: 2976.48

=== LANGUAGE es ===

=== Perplexity result ===
Model: base
Average tokenâ€‘wise loss : 1.4418
Perplexity (exp(loss)) : 4.23
No NaNs
No Infs
ðŸ”Ž Total L2 norm of parameters: 2997.18

=== Perplexity result ===
Model: language_es
Average tokenâ€‘wise loss : 1.3028
Perplexity (exp(loss)) : 3.68
No NaNs
No InfsTotal L2 norm of parameters: 2974.82

=== LANGUAGE hi ===

=== Perplexity result ===
Model: base
Average tokenâ€‘wise loss : 1.2975
Perplexity (exp(loss)) : 3.66
No NaNs
No Infs
ðŸ”Ž Total L2 norm of parameters: 2997.18

=== Perplexity result ===
Model: language_hi
Average tokenâ€‘wise loss : 1.8217
Perplexity (exp(loss)) : 6.18
No NaNs
No InfsTotal L2 norm of parameters: 2975.48

=== LANGUAGE de ===

=== Perplexity result ===
Model: base
Average tokenâ€‘wise loss : 2.1038
Perplexity (exp(loss)) : 8.20
No NaNs
No Infs
ðŸ”Ž Total L2 norm of parameters: 2997.18

=== Perplexity result ===
Model: language_de
Average tokenâ€‘wise loss : 1.9400
Perplexity (exp(loss)) : 6.96
No NaNs
No InfsTotal L2 norm of parameters: 2976.92

=== LANGUAGE zh ===

=== Perplexity result ===
Model: base
Average tokenâ€‘wise loss : 1.7740
Perplexity (exp(loss)) : 5.89
No NaNs
No Infs
ðŸ”Ž Total L2 norm of parameters: 2997.18

=== Perplexity result ===
Model: language_zh
Average tokenâ€‘wise loss : 1.8764
Perplexity (exp(loss)) : 6.53
No NaNs
No InfsTotal L2 norm of parameters: 2974.20
