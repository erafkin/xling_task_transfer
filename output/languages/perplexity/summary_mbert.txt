
=== LANGUAGE en ===

=== Perplexity ===
Model: base
Average tokenâ€‘wise loss : 2.5744
Perplexity (exp(loss)) : 13.12
No NaNs
No Infs
ðŸ”Ž Total L2 norm of parameters: 584.84

=== Perplexity result ===
Model: language_en
Average tokenâ€‘wise loss : 1.8650
Perplexity (exp(loss)) : 6.46
No NaNs
No InfsTotal L2 norm of parameters: 581.55

=== LANGUAGE es ===

=== Perplexity ===
Model: base
Average tokenâ€‘wise loss : 1.4210
Perplexity (exp(loss)) : 4.14
No NaNs
No Infs
ðŸ”Ž Total L2 norm of parameters: 584.84

=== Perplexity result ===
Model: language_es
Average tokenâ€‘wise loss : 0.7655
Perplexity (exp(loss)) : 2.15
No NaNs
No InfsTotal L2 norm of parameters: 581.13

=== LANGUAGE hi ===

=== Perplexity ===
Model: base
Average tokenâ€‘wise loss : 0.9423
Perplexity (exp(loss)) : 2.57
No NaNs
No Infs
ðŸ”Ž Total L2 norm of parameters: 584.84

=== Perplexity result ===
Model: language_hi
Average tokenâ€‘wise loss : 0.8829
Perplexity (exp(loss)) : 2.42
No NaNs
No InfsTotal L2 norm of parameters: 581.16

=== LANGUAGE de ===

=== Perplexity ===
Model: base
Average tokenâ€‘wise loss : 1.7361
Perplexity (exp(loss)) : 5.68
No NaNs
No Infs
ðŸ”Ž Total L2 norm of parameters: 584.84

=== Perplexity result ===
Model: language_de
Average tokenâ€‘wise loss : 1.3894
Perplexity (exp(loss)) : 4.01
No NaNs
No InfsTotal L2 norm of parameters: 581.51

=== LANGUAGE zh ===

=== Perplexity ===
Model: base
Average tokenâ€‘wise loss : 1.4294
Perplexity (exp(loss)) : 4.18
No NaNs
No Infs
ðŸ”Ž Total L2 norm of parameters: 584.84

=== Perplexity result ===
Model: language_zh
Average tokenâ€‘wise loss : 0.8312
Perplexity (exp(loss)) : 2.30
No NaNs
No InfsTotal L2 norm of parameters: 581.28
