
=== LANGUAGE en ===

=== Perplexity ===
Model: base
Average tokenâ€‘wise loss : 3.3003
Perplexity (exp(loss)) : 27.12
No NaNs
No Infs
ðŸ”Ž Total L2 norm of parameters: 584.84

=== Perplexity result ===
Model: language_en
Average tokenâ€‘wise loss : 2.8178
Perplexity (exp(loss)) : 16.74
No NaNs
No InfsTotal L2 norm of parameters: 581.55

=== LANGUAGE es ===

=== Perplexity ===
Model: base
Average tokenâ€‘wise loss : 2.6220
Perplexity (exp(loss)) : 13.76
No NaNs
No Infs
ðŸ”Ž Total L2 norm of parameters: 584.84

=== Perplexity result ===
Model: language_es
Average tokenâ€‘wise loss : 2.2206
Perplexity (exp(loss)) : 9.21
No NaNs
No InfsTotal L2 norm of parameters: 581.13

=== LANGUAGE hi ===

=== Perplexity ===
Model: base
Average tokenâ€‘wise loss : 1.9605
Perplexity (exp(loss)) : 7.10
No NaNs
No Infs
ðŸ”Ž Total L2 norm of parameters: 584.84

=== Perplexity result ===
Model: language_hi
Average tokenâ€‘wise loss : 1.6128
Perplexity (exp(loss)) : 5.02
No NaNs
No InfsTotal L2 norm of parameters: 581.16

=== LANGUAGE de ===

=== Perplexity ===
Model: base
Average tokenâ€‘wise loss : 3.0307
Perplexity (exp(loss)) : 20.71
No NaNs
No Infs
ðŸ”Ž Total L2 norm of parameters: 584.84

=== Perplexity result ===
Model: language_de
Average tokenâ€‘wise loss : 2.6789
Perplexity (exp(loss)) : 14.57
No NaNs
No InfsTotal L2 norm of parameters: 581.51

=== LANGUAGE zh ===

=== Perplexity ===
Model: base
Average tokenâ€‘wise loss : 2.2965
Perplexity (exp(loss)) : 9.94
No NaNs
No Infs
ðŸ”Ž Total L2 norm of parameters: 584.84

=== Perplexity result ===
Model: language_zh
Average tokenâ€‘wise loss : 2.0870
Perplexity (exp(loss)) : 8.06
No NaNs
No InfsTotal L2 norm of parameters: 581.28

=== LANGUAGE fr ===

=== Perplexity ===
Model: base
Average tokenâ€‘wise loss : 2.5283
Perplexity (exp(loss)) : 12.53
No NaNs
No Infs
ðŸ”Ž Total L2 norm of parameters: 584.84

=== Perplexity result ===
Model: language_fr
Average tokenâ€‘wise loss : 2.1145
Perplexity (exp(loss)) : 8.29
No NaNs
No InfsTotal L2 norm of parameters: 581.29

=== LANGUAGE ru ===

=== Perplexity ===
Model: base
Average tokenâ€‘wise loss : 2.3459
Perplexity (exp(loss)) : 10.44
No NaNs
No Infs
ðŸ”Ž Total L2 norm of parameters: 584.84

=== Perplexity result ===
Model: language_ru
Average tokenâ€‘wise loss : 1.9390
Perplexity (exp(loss)) : 6.95
No NaNs
No InfsTotal L2 norm of parameters: 581.11
