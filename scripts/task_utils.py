from torch import nn
import pandas as pd
import re
from pathlib import Path
from datasets import Dataset

class TokenClassificationHead(nn.Module):
    def __init__(self, encoder: nn.Module, num_labels: int, dropout: float = 0.1, bert:bool=False):
        super().__init__()
        self.roberta = None
        self.bert = None
        self.classifier = None
        if bert:
            self.bert = encoder
            self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)
        else:
            self.roberta = encoder
            self.classifier = nn.Linear(self.roberta.config.hidden_size, num_labels)
        self.dropout = nn.Dropout(dropout)
        
        nn.init.xavier_uniform_(self.classifier.weight)
        nn.init.zeros_(self.classifier.bias)

    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None, **kwargs):
        if self.bert is None:
            outputs = self.roberta(
                input_ids,
                attention_mask=attention_mask,
                token_type_ids=token_type_ids,
                return_dict=True,
            )
        else:
            outputs = self.bert(
            input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            return_dict=True,
        )
        logits = self.classifier(self.dropout(outputs.last_hidden_state))

        loss = None
        if labels is not None:
            loss_fct = nn.CrossEntropyLoss()
            loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))

        return {"loss": loss, "logits": logits}
    
def load_conllu_data(filepath) -> pd.DataFrame:
    """
    Loads data from a CoNLL-U file and structures it into a list of sentences,
    where each sentence is a list of token dictionaries. Initially generated by GoogleAI
    """
    sentences = []
    current_sentence = []

    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line:  # Empty line indicates end of a sentence
                if current_sentence:
                    sentences.append([[t["FORM"] for t in current_sentence],
                          [t["UPOS"] for t in current_sentence],
                          [t["DEPREL"] for t in current_sentence],
                          [t["HEAD"] for t in current_sentence]])
                    current_sentence = []
                continue

            if line.startswith('#'):  # Skip comments
                continue

            # Split the line into fields according to CoNLL-U format
            # ID, FORM, LEMMA, UPOS, XPOS, FEATS, HEAD, DEPREL, DEPS, MISC
            fields = line.split('\t')
            if len(fields) != 10:
                # Handle potential errors or malformed lines
                continue

            token = {
                'ID': fields[0],
                'FORM': fields[1],
                'LEMMA': fields[2],
                'UPOS': fields[3],
                'XPOS': fields[4],
                'FEATS': fields[5],
                'HEAD': fields[6],
                'DEPREL': fields[7],
                'DEPS': fields[8],
                'MISC': fields[9]
            }
            current_sentence.append(token)

    # Add the last sentence if the file doesn't end with an empty line
    if current_sentence:
        sentences.append([[t["FORM"] for t in current_sentence],
                          [t["UPOS"] for t in current_sentence],
                          [t["DEPREL"] for t in current_sentence],
                          [t["HEAD"] for t in current_sentence]])
    rows = []
    df = pd.DataFrame(sentences, columns=["tokens", "pos_tags", "dep_rel", "dep_head"])
    return df

def read_uner(file_path):
    """
        Read Universal NER file.

        Code from the Unversal NER repository: https://github.com/UniversalNER/uner_code/blob/master/prepare_data.py#L7C1-L32C19
    """
    file_path = Path(file_path)
    raw_text = file_path.read_text().strip()
    raw_docs = re.split(r'\n\t?\n', raw_text)

    token_docs = []
    tag_docs = []
    for doc in raw_docs:
        tokens = []
        tags = []
        for line in doc.split('\n'):
            # ignore comments
            if line.startswith('#'): continue
            tok_id, token, tag = line.split('\t')[:3]
            tokens.append(token)
            if "OTH" in tag or tag == "B-O":
                tag = "O"
            tags.append(tag)
        token_docs.append(tokens)
        tag_docs.append(tags)


    train_dict = {"tokens": token_docs, "ner_tags": tag_docs}
    dataset = Dataset.from_dict(train_dict)
    return dataset
