from transformers import (
    AutoModelForMaskedLM, 
    AutoTokenizer, 
    AutoModelForSequenceClassification,
    DataCollatorForTokenClassification,
    DataCollatorWithPadding,
    TrainingArguments, 
    AutoConfig,
    Trainer
)
import torch
from datasets import load_dataset, DatasetDict, Dataset
import argparse
import numpy as np
from torch import nn
import pandas as pd

class TokenClassificationHead(nn.Module):
    def __init__(self, encoder: nn.Module, num_labels: int, dropout: float = 0.1):
        super().__init__()
        self.bert = encoder
        self.dropout = nn.Dropout(dropout)
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)
        nn.init.xavier_uniform_(self.classifier.weight)
        nn.init.zeros_(self.classifier.bias)

    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None, **kwargs):
        outputs = self.bert(
            input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            return_dict=True,
        )
        logits = self.classifier(self.dropout(outputs.last_hidden_state))

        loss = None
        if labels is not None:
            loss_fct = nn.CrossEntropyLoss()
            loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))

        return {"loss": loss, "logits": logits}
    
def load_conllu_data(filepath) -> pd.DataFrame:
    """
    Loads data from a CoNLL-U file and structures it into a list of sentences,
    where each sentence is a list of token dictionaries. Initially generated by GoogleAI
    """
    sentences = []
    current_sentence = []

    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line:  # Empty line indicates end of a sentence
                if current_sentence:
                    sentences.append([[t["FORM"] for t in current_sentence],
                          [t["UPOS"] for t in current_sentence],
                          [t["DEPREL"] for t in current_sentence],
                          [t["HEAD"] for t in current_sentence]])
                    current_sentence = []
                continue

            if line.startswith('#'):  # Skip comments
                continue

            # Split the line into fields according to CoNLL-U format
            # ID, FORM, LEMMA, UPOS, XPOS, FEATS, HEAD, DEPREL, DEPS, MISC
            fields = line.split('\t')
            if len(fields) != 10:
                # Handle potential errors or malformed lines
                continue

            token = {
                'ID': fields[0],
                'FORM': fields[1],
                'LEMMA': fields[2],
                'UPOS': fields[3],
                'XPOS': fields[4],
                'FEATS': fields[5],
                'HEAD': fields[6],
                'DEPREL': fields[7],
                'DEPS': fields[8],
                'MISC': fields[9]
            }
            current_sentence.append(token)

    # Add the last sentence if the file doesn't end with an empty line
    if current_sentence:
        sentences.append([[t["FORM"] for t in current_sentence],
                          [t["UPOS"] for t in current_sentence],
                          [t["DEPREL"] for t in current_sentence],
                          [t["HEAD"] for t in current_sentence]])
    rows = []
    df = pd.DataFrame(sentences, columns=["tokens", "pos_tags", "dep_rel", "dep_head"])
    return df


def train_NER_model(model_checkpoint):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
    data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)
    NER_dataset = load_dataset("MultiCoNER/multiconer_v2", "English (EN)", trust_remote_code=True)
    # Build a dense mapping 0 … C‑1
    unique_tags = sorted({tag for ex in NER_dataset["test"] for tag in ex["ner_tags"]})
    label2id = {tag: i for i, tag in enumerate(unique_tags)}
    id2label = {i: tag for tag, i in label2id.items()}
    def tokenize_and_align_labels(examples):
        # from https://reybahl.medium.com/token-classification-in-python-with-huggingface-3fab73a6a20e
        tokenized_inputs = tokenizer(examples["tokens"], truncation=True, is_split_into_words=True)
        labels = []
        for i, label in enumerate(examples[f"ner_tags"]):
            word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.
            previous_word_idx = None
            label_ids = []
            for word_idx in word_ids:  # Set the special tokens to -100.
                if word_idx is None:
                    label_ids.append(-100)
                elif word_idx != previous_word_idx:  # Only label the first token of a given word.
                    label_ids.append(label2id[label[word_idx]])
                else:
                    label_ids.append(-100)
                previous_word_idx = word_idx
            labels.append(label_ids)

        tokenized_inputs["labels"] = labels
        return tokenized_inputs
    def compute_metrics(eval_pred):
        predictions, labels = eval_pred
        predictions = np.argmax(predictions, axis=2)

        # Remove ignored labels (-100)
        true_predictions = [
            [p for (p, l) in zip(prediction, label) if l != -100]
            for prediction, label in zip(predictions, labels)
        ]
        true_labels = [
            [l for (p, l) in zip(prediction, label) if l != -100]
            for prediction, label in zip(predictions, labels)
        ]

        # Simple accuracy calculation
        total = sum(len(pred) for pred in true_predictions)
        correct = sum(1 for pred, lab in zip(true_predictions, true_labels) for p, l in zip(pred, lab) if p == l)
        accuracy = correct / total if total > 0 else 0
        return {"accuracy": accuracy}

    tokenized_dataset= NER_dataset.map(
        tokenize_and_align_labels,
        batched=True,
        remove_columns=NER_dataset["test"].column_names
    )

    config = AutoConfig.from_pretrained(model_checkpoint)
    mlm_model = AutoModelForMaskedLM.from_pretrained(
        model_checkpoint,
        config=config,
        dtype=torch.float32,
    ).to(device)

    bert_encoder = mlm_model.roberta
    model = TokenClassificationHead(
        encoder=bert_encoder,
        num_labels=len(id2label),
        dropout=0.1,
    ).to(device)

    def set_trainable(mod: nn.Module, train_encoder: bool = False):
        for n, p in mod.named_parameters():
            if "classifier" in n:
                p.requires_grad = True
            else:
                p.requires_grad = train_encoder
        mod.train()

    set_trainable(model, train_encoder=True) 

    training_args = TrainingArguments(
            output_dir=f"NER_en",
            eval_strategy="epoch",
            learning_rate=2e-5,
            num_train_epochs=3, 
            weight_decay=0.01,
            per_device_train_batch_size=8,
            per_device_eval_batch_size=8,
            push_to_hub=False,
            save_strategy="no",
            fp16=False
        )    
    trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized_dataset["test"],
            eval_dataset=tokenized_dataset["validation"],
            data_collator=data_collator,
            tokenizer=tokenizer,
            compute_metrics=compute_metrics,
        )

    trainer.train()
    trainer.save_model(f"NER_en")

def train_POS_model(model_checkpoint, GUM_folder: str = "GUM_en"):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    train_dataset = load_conllu_data(f"{GUM_folder}/en_gum-ud-train.conllu")
    dev_dataset = load_conllu_data(f"{GUM_folder}/en_gum-ud-dev.conllu")
    dataset = DatasetDict({"train": Dataset.from_pandas(train_dataset), "dev": Dataset.from_pandas(dev_dataset)})
    unique_tags = sorted({tag for ex in dataset["train"] for tag in ex["pos_tags"]})
    label2id = {tag: i for i, tag in enumerate(unique_tags)}
    id2label = {i: tag for tag, i in label2id.items()}
    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
    data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)
    def tokenize_and_align_labels(examples):
        # from https://reybahl.medium.com/token-classification-in-python-with-huggingface-3fab73a6a20e
        tokenized_inputs = tokenizer(examples["tokens"], truncation=True, is_split_into_words=True)
        labels = []
        for i, label in enumerate(examples["pos_tags"]):
            word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.
            previous_word_idx = None
            label_ids = []
            for word_idx in word_ids:  # Set the special tokens to -100.
                if word_idx is None:
                    label_ids.append(-100)
                elif word_idx != previous_word_idx:  # Only label the first token of a given word.
                    label_ids.append(label2id[label[word_idx]])
                else:
                    label_ids.append(-100)
                previous_word_idx = word_idx
            labels.append(label_ids)

        tokenized_inputs["labels"] = labels
        return tokenized_inputs
    def compute_metrics(eval_pred):
        predictions, labels = eval_pred
        predictions = np.argmax(predictions, axis=2)

        # Remove ignored labels (-100)
        true_predictions = [
            [p for (p, l) in zip(prediction, label) if l != -100]
            for prediction, label in zip(predictions, labels)
        ]
        true_labels = [
            [l for (p, l) in zip(prediction, label) if l != -100]
            for prediction, label in zip(predictions, labels)
        ]

        # Simple accuracy calculation
        total = sum(len(pred) for pred in true_predictions)
        correct = sum(1 for pred, lab in zip(true_predictions, true_labels) for p, l in zip(pred, lab) if p == l)
        accuracy = correct / total if total > 0 else 0
        return {"accuracy": accuracy}

    tokenized_dataset= dataset.map(
        tokenize_and_align_labels,
        batched=True,
        remove_columns=["tokens", "pos_tags", "dep_rel", "dep_head"]
    )

    config = AutoConfig.from_pretrained(model_checkpoint)
    mlm_model = AutoModelForMaskedLM.from_pretrained(
        model_checkpoint,
        config=config,
        dtype=torch.float32,
    ).to(device)

    bert_encoder = mlm_model.roberta
    model = TokenClassificationHead(
        encoder=bert_encoder,
        num_labels=len(id2label),
        dropout=0.1,
    ).to(device)

    def set_trainable(mod: nn.Module, train_encoder: bool = False):
        for n, p in mod.named_parameters():
            if "classifier" in n:
                p.requires_grad = True
            else:
                p.requires_grad = train_encoder
        mod.train()

    set_trainable(model, train_encoder=True) 

    training_args = TrainingArguments(
            output_dir=f"POS_en",
            eval_strategy="epoch",
            learning_rate=2e-5,
            num_train_epochs=3, 
            weight_decay=0.01,
            per_device_train_batch_size=8,
            per_device_eval_batch_size=8,
            push_to_hub=False,
            save_strategy="no",
            fp16=False
        )    
    trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized_dataset["train"],
            eval_dataset=tokenized_dataset["dev"],
            data_collator=data_collator,
            tokenizer=tokenizer,
            compute_metrics=compute_metrics,
        )

    trainer.train()
    trainer.save_model(f"POS_en")


def train_NLI_model(model_checkpoint):
    # Following Ansell: https://github.com/cambridgeltl/composable-sft/blob/main/examples/text-classification/run_text_classification.py
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
    NLI_dataset = load_dataset("facebook/xnli", "en", trust_remote_code=True)
    # Build a dense mapping 0 … C‑1
    unique_tags = sorted({ex["label"] for ex in NLI_dataset["train"]})
    label2id = {tag: i for i, tag in enumerate(unique_tags)}
    id2label = {i: tag for tag, i in label2id.items()}
    def preprocess(examples):
        inputs = [examples[column] for column in ["premise", "hypothesis"]]
        tokenized_examples = tokenizer(
            *inputs,
            padding=True,
            max_length=512,
            truncation=True,
        )
        tokenized_examples['label'] = [
            label2id[str(label)]
            for label in examples['label']
        ]
        return tokenized_examples
    def compute_metrics(eval_pred):
        predictions, labels = eval_pred
        predictions = np.argmax(predictions, axis=2)
        # Simple accuracy calculation
        total = sum(len(pred) for pred in predictions)
        correct = sum(1 for pred, lab in zip(predictions, labels) for p, l in zip(pred, lab) if p == l)
        accuracy = correct / total if total > 0 else 0
        return {"accuracy": accuracy}

    tokenized_dataset= NLI_dataset.map(
        preprocess,
        batched=True,
        remove_columns=NLI_dataset["train"].column_names
    )

    config = AutoConfig.from_pretrained(model_checkpoint)
    model = AutoModelForSequenceClassification.from_pretrained(
        model_checkpoint,
        config=config,
        dtype=torch.float32,
        num_labels=len(id2label),
    ).to(device)

    def set_trainable(mod: nn.Module, train_encoder: bool = False):
        for n, p in mod.named_parameters():
            if "classifier" in n:
                p.requires_grad = True
            else:
                p.requires_grad = train_encoder
        mod.train()

    set_trainable(model, train_encoder=True) 

    training_args = TrainingArguments(
            output_dir=f"NLI_en",
            eval_strategy="epoch",
            learning_rate=2e-5,
            num_train_epochs=3, 
            weight_decay=0.01,
            per_device_train_batch_size=8,
            per_device_eval_batch_size=8,
            push_to_hub=False,
            save_strategy="no",
            fp16=False
        )    
    trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized_dataset["train"],
            eval_dataset=tokenized_dataset["validation"],
            data_collator=data_collator,
            tokenizer=tokenizer,
            compute_metrics=compute_metrics,
        )

    trainer.train()
    trainer.save_model(f"NLI_en")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="A simple script to demonstrate argument parsing.")
    parser.add_argument("task", help="the task to train an english model on")
    args = parser.parse_args()
    if args.task == "ner":
        train_NER_model("bert-multlingual/language_en_done")
    elif args.task == "pos":
        train_POS_model("bert-multlingual/language_en_done")
    elif args.task == "nli":
        train_NLI_model("bert-multlingual/language_en_done")
    else:
        print("no task: ", args.task)
